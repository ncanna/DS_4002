# -*- coding: utf-8 -*-
"""Albini_Annaparedy_Sarnaik_Vellayan_DS4002_W2_Code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YMMb8j8DGDGqH_fikY9T03onU3Z2D8Mt

# Data Importing and Cleaning
"""

import pandas as pd
import numpy as np
from os import listdir, walk
from os.path import isfile, join
import itertools
import sys,requests
import plotly.express as px
import plotly.graph_objects as go
import plotly.figure_factory as ff
from plotly.offline import iplot, init_notebook_mode
import matplotlib.pyplot as plt
import seaborn as sns
import math 
import cv2
import pickle
from collections import Counter

#USE IF CONVERTING TO HTML
# import plotly.offline as py
# py.init_notebook_mode(connected=False)

from sklearn.feature_selection import chi2
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import LabelEncoder
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split 
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.inspection import permutation_importance
from sklearn.model_selection import cross_val_score
from sklearn.metrics import classification_report, confusion_matrix
import sklearn.metrics as skmet
pd.set_option('mode.chained_assignment', None) 

import keras
import keras.utils
from keras.models import Model
from keras.applications.resnet50 import ResNet50
from keras.applications.vgg16 import VGG16
from keras.preprocessing import image
from keras.preprocessing.image import load_img
from keras.applications.resnet50 import preprocess_input, decode_predictions
from keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
from sklearn.utils import class_weight
from keras.layers.convolutional import Conv2D, MaxPooling2D
from keras.models import Sequential
from keras.layers import Dense, Activation, Flatten, Dropout
from keras.optimizers import SGD
from keras import utils as np_utils
import tensorflow as tf

class history(Callback):
    def __init__(self, savepath):
        super(history, self).__init__()
        self.savepath = savepath
        self.history = {}
    def on_epoch_end(self, epoch, logs=None):
        for k, v in logs.items():
            self.history.setdefault(k, []).append(v)
        np.save(self.savepath, self.history)

# Load the Drive helper and mount
from google.colab import drive
drive.mount('/content/drive')
#!ls "/content/drive/MyDrive/DS_4002_JTerm2021/Week Two/Code/data/sample_images"

folder_path = "/content/drive/MyDrive/DS_4002_JTerm2021/Week Two/Code/data/sample_images/images"
file_path = "/content/drive/MyDrive/DS_4002_JTerm2021/Week Two/Code/data/sample_images/sample_labels.csv"
folder_path_append = "/content/drive/MyDrive/DS_4002_JTerm2021/Week Two/Code/data/sample_images/images/"
pkl_path = '/content/drive/MyDrive/DS_4002_JTerm2021/Week Two/Code/img_arrays.pkl'
weight_path = '/content/drive/MyDrive/DS_4002_JTerm2021/Week Two/Code/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5' 
#Weight source: https://github.com/zdata-inc/applied_deep_learning/blob/master/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5 

recode_classes={0: 'No Finding', 1: 'Single Finding', 2: 'Multiple Findings'}
recoded_list = list(recode_classes.values())

def getAllFilesInDirectory(directoryPath: str):
    return [(directoryPath + "/" + f) for f in listdir(directoryPath) if isfile(join(directoryPath, f))]

df = pd.read_csv(file_path)
df["path"] = folder_path_append+df["Image Index"]
print(df.shape)

categorized_df = df.copy()
categorized_df["Class"] = ""
categorized_df["Class"][(categorized_df["Finding Labels"] == "No Finding")] = 0
categorized_df["Class"][categorized_df["Finding Labels"].str.contains("|", regex=False)] = 2
categorized_df["Class"][(categorized_df["Class"] != 0) & (categorized_df["Class"] != 2)] = 1
categorized_df["Class"] = categorized_df["Class"].astype('int32')
categorized_df

findings = ["No Finding", "Infiltration", "Effusion", "Atelectasis", "Nodule", "Pneumothorax", "Mass"]
is_finding = categorized_df["Finding Labels"].isin(findings)
target_findings = categorized_df[is_finding].reset_index()
recoded_df = categorized_df[['Finding Labels', 'path', 'Class']]
print(recoded_df.shape)
categorized_df.head(1)

categorized_df.Class.value_counts()

recoded_df.Class.value_counts()

"""# Label Analysis"""

target_data = df["Finding Labels"].value_counts().rename_axis('Finding').reset_index(name='Count').head(7)
target_data

print(target_data.Count.sum())
target_data.Count.sum()/len_all_data

target_findings["Finding Labels"].value_counts().rename_axis('Finding').reset_index(name='Count')

binary_df = df.copy()
binary_df["Finding Labels"][binary_df["Finding Labels"] != "No Finding"] = "Finding"
binary_df["Finding Labels"].value_counts().rename_axis('Finding').reset_index(name='Count')

num_normal = (binary_df["Finding Labels"] == "No Finding").sum()
num_abnormal = (binary_df["Finding Labels"] == "Finding").sum()

layout = go.Layout(title='Original Base Rates of Target Classes (Findings)')
fig = go.Figure(data=[go.Pie(labels=["No Finding", "Finding"],
                             values=[num_normal, num_abnormal],
                             pull=[0, 0.1],
                             )], layout=layout)
fig.update_traces(hoverinfo='label+value', textinfo='percent', textfont_size=20,
                  marker=dict(line=dict(color='#000000', width=2)))
fig.show()

all_finding_labels = df.groupby(["Finding Labels"]).size().reset_index(name='Count')
fig = px.bar(all_finding_labels,
             x="Finding Labels",
             y="Count",
             color_discrete_sequence=px.colors.qualitative.D3, 
             title='Distribution of all Finding Labels'
             )
fig.show()

tertiary_df = df.copy()
tertiary_df["Finding Labels"][tertiary_df["Finding Labels"].str.contains("|", regex=False)] = "Multiple Findings"
tertiary_df["Finding Labels"][(tertiary_df["Finding Labels"] != "No Finding") & (tertiary_df["Finding Labels"] != "Multiple Findings")] = "One Finding"
tertiary_df["Finding Labels"].value_counts().rename_axis('Finding').reset_index(name='Count')

num_nofinding = (tertiary_df["Finding Labels"] == "No Finding").sum()
num_onefinding = (tertiary_df["Finding Labels"] == "One Finding").sum()
num_mulfinding = (tertiary_df["Finding Labels"] == "Multiple Findings").sum()

layout = go.Layout(title='Original Base Rates of Target Classes')
fig = go.Figure(data=[go.Pie(labels=["No Finding", "One Finding", "Multiple Findings"],
                             values=[num_nofinding, num_onefinding, num_mulfinding],
                             pull=[0.1, 0, 0],
                             )], layout=layout)

fig.update_traces(hoverinfo='label+value', textinfo='percent', textfont_size=20,
                  marker=dict(line=dict(color='#000000', width=2)))
fig.show()

num_no_findings = (df['Finding Labels'] == "No Finding").sum()
num_keep = (target_findings["Finding Labels"] != "No Finding").sum()
num_drop = 5606 - 4299

layout = go.Layout(title='Dropped vs. Retained Data')
fig = go.Figure(data=[go.Pie(labels=["Kept with No Findings", "Kept with Findings", "Dropped"],
                             values=[num_no_findings, num_keep, num_drop],
                             pull=[0, 0, 0.1],
                             )], layout=layout)
fig.update_traces(hoverinfo='label+value', textinfo='percent', textfont_size=20,
                  marker=dict(line=dict(color='#000000', width=2)))
fig.show()

tertiary_df = target_findings.copy()
tertiary_df["Finding Labels"][tertiary_df["Finding Labels"].str.contains("|", regex=False)] = "Multiple Findings"
tertiary_df["Finding Labels"][(tertiary_df["Finding Labels"] != "No Finding") & (tertiary_df["Finding Labels"] != "Multiple Findings")] = "One Finding"
tertiary_df["Finding Labels"].value_counts().rename_axis('Finding').reset_index(name='Count')

num_nofinding = (tertiary_df["Finding Labels"] == "No Finding").sum()
num_onefinding = (tertiary_df["Finding Labels"] == "One Finding").sum()
num_mulfinding = (tertiary_df["Finding Labels"] == "Multiple Findings").sum()

layout = go.Layout(title='Filtered Base Rates of Target Classes')
fig = go.Figure(data=[go.Pie(labels=["No Finding", "One Finding", "Multiple Findings"],
                             values=[num_nofinding, num_onefinding, num_mulfinding],
                             pull=[0.1, 0, 0],
                             )], layout=layout)

fig.update_traces(hoverinfo='label+value', textinfo='percent', textfont_size=20,
                  marker=dict(line=dict(color='#000000', width=2)))
fig.show()

all_finding_labels = target_findings.groupby(["Finding Labels"]).size().reset_index(name='Count')
fig = px.pie(all_finding_labels,
             values="Count",
             names = "Finding Labels",
             color_discrete_sequence=px.colors.qualitative.D3, 
             title='Distribution of Filtered Findings Labels'
             )
fig.show()

all_finding_labels = target_findings.groupby(["Finding Labels"]).size().reset_index(name='Count')
fig = px.bar(all_finding_labels,
             x="Finding Labels",
             y="Count",
             color_discrete_sequence=px.colors.qualitative.D3, 
             title='Distribution of Filtered Findings Labels'
             )
fig.show()

comp_g = target_findings.groupby(["Finding Labels", "Patient Gender"]).size().reset_index(name='Count')
fig = px.bar(comp_g,
             x="Finding Labels",
             y="Count",
             color="Patient Gender",
             barmode='group',
             color_discrete_sequence=px.colors.qualitative.D3,
             title='Distribution of Filtered Findings Labels by Gender')

fig.show()

"""# Plotting functions"""

def plot_single(image_path):
  path = image_path
  print(image_path)
  image_id = path.split('/')[10]
  row = df[df['Image Index'] == image_id].values[0]
  label = str(row[1])
  img = image.load_img(image_path, target_size=(200, 200))
  plt.title(image_id+"\n"+label)
  plt.imshow(img)
  plt.axis('off')
  plt.show()

def plot_index(index, DataFrame):
  row = DataFrame.iloc[index].values
  image_id = str(row[0])
  label = str(row[1])
  image_path = str(row[11])
  print(image_path)
  img = image.load_img(image_path, target_size=(200, 200))
  plt.title(image_id+"\n"+label)
  plt.imshow(img)
  plt.axis('off')
  plt.show()
  img1 = cv2.imread(image_path,0)
  plt.hist(img1.ravel(),256,[0,256])
  plt.title("Pixel intensity vs. number of pixels")
  plt.show()
  histg = cv2.calcHist([img1],[0],None,[256],[0,256])
  plt.plot(histg) 
  plt.title("Pixel intensity vs. number of pixels")
  plt.show()

plot_index(10, df)

plot_index(41, df)

plot_index(229, df)

recoded_df[recoded_df["Class"] == 2]

"""# Feature engineering

## Sampling target data
"""

recoded_df_sample = recoded_df.head(5)

plotted_cv2 = cv2.resize(np.array(cv2.imread(recoded_df_sample.path[0], cv2.COLOR_BGR2GRAY)), (128,128))
plt.imshow(plotted_cv2, cmap = 'gray', interpolation = 'bicubic')
print(plotted_cv2.shape)

plotted_cv2 = cv2.resize(np.array(cv2.imread(recoded_df_sample.path[0])), (128,128))
plt.imshow(plotted_cv2, cmap = 'gray', interpolation = 'bicubic')
print(plotted_cv2.shape)

"""## Turning images into arrays"""

def image_to_array(df, img_width, img_height):
  i = 0
  target_df = pd.DataFrame()
  img_array, labels = [], [] 
  for index, row in df.iterrows():
    figure = cv2.resize(np.array(cv2.imread(row[1])), (img_width,img_height))
    img_array.append(figure) 
    labels.append(row[2])
    i += 1
  target_df["images"] = img_array
  target_df["labels"] = labels
  return target_df

target_df = image_to_array(recoded_df, 128, 128)
picklefile = open(pkl_path, 'wb')
pickle.dump(target_df, picklefile)
picklefile.close()

print(target_df.shape)
print(target_df.images[0].shape)

target_df.head(3)

"""## Loading pickled image arrays"""

picklefile = open(pkl_path, 'rb')
target_df = pickle.load(picklefile)
target_df.shape

target_df.labels.value_counts()

target_df.columns

"""## Defining original weights train test split"""

X,y = np.asarray(target_df['images']), np.asarray(target_df['labels'])

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

print(f'X_train is {len(X_train)} and X_test is {len(X_test)}')
print(f'Y_train is {len(y_train)} and Y_test is {len(y_test)}')

# Block training for testing

block_train = True

if block_train:
  X_train = X_train[0:80] 
  y_train = y_train[0:80]
  X_test = X_test[0:20] 
  y_test = y_test[0:20]

print(f'X_train is {len(X_train)} and X_test is {len(X_test)}')
print(f'Y_train is {len(y_train)} and Y_test is {len(y_test)}')

print(len(target_df))
print(target_df.images[0].shape)
print(type(X))
print(type(y))

target_df.images[0].shape

label_encoded_array = np.unique(y)
unbalanced_weights = class_weight.compute_class_weight('balanced', np.unique(y), y)
weight_data = {'labels': label_encoded_array, 'weight': unbalanced_weights}
pd.DataFrame(data=weight_data)

unbalanced_weights

"""# Training """

def model(X_train,y_train,X_test,y_test,input_weights):
    num_class = 3
    epochs = 1
    base_model = VGG16(weights = weight_path, include_top=False, input_shape=(128, 128, 3))

    # Add a new top layer
    x = base_model.output
    x = Flatten()(x)

    # SSoftmax
    prediction_layer = Dense(num_class, activation='softmax')(x)

    model = Model(inputs=base_model.input, outputs=predictions)

    # Do not train base layers; only top
    for layer in base_model.layers:
        layer.trainable = False

    # Compiler = categorical_crossentropy
    model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.RMSprop(lr=0.0001), metrics=['accuracy'])
    callbacks_list = [keras.callbacks.EarlyStopping(monitor='val_acc', patience=3, verbose=1)]
    model.summary()

    # Fitting model
    model.fit(X_train,y_train, epochs=epochs, class_weight=input_weights, validation_data=(X_test,y_test), verbose=1,callbacks = [history('metrics')])
    modelScore = model.evaluate(X_test,y_test, verbose=0)

    # Printing metrics
    print(f'Accuracy: {modelScore[1]}')
    y_pred = model.predict(X_test)
    print(sklearn.metrics.classification_report(np.where(d > 0)[1], np.argmax(y_pred, axis=1), target_names=recoded_list)) 

    # Printing confusion matrix
    true_predictions = np.argmax(y_test,axis = 1) 
    predictions = np.argmax(y_pred,axis = 1) 
    confusion_mtx = confusion_matrix(true_predictions, predictions) 

    # Plotting train and test accuracies
    plt.figure(figsize=(10,5))
    metrics = np.load('metrics.npy')[()]
    metric_list = ['acc', 'loss'] 
    for k in filter(lambda x : np.any([metric in x for metric in metric_list]), metrics.keys()):
        l = np.array(metrics[k])
        plt.plot(l, c= 'r' if 'val' not in k else 'b', label='val' if 'val' in k else 'train')
        x = np.argmin(l) if 'loss' in k else np.argmax(l)
        y = l[x]
        plt.scatter(x,y, lw=0, alpha=0.25, s=100, c='r' if 'val' not in k else 'b')
        plt.text(x, y, '{} = {:.4f}'.format(x,y), size='15', color= 'r' if 'val' not in k else 'b')   
    plt.legend(loc=4)
    plt.axis([0, None, None, None]);
    plt.grid()
    plt.xlabel('Epochs')
    plt.show()
    return model

weight_path

model(X_train, y_train, X_test, y_test,weight_path,3)